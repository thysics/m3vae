{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159f3f63",
   "metadata": {},
   "source": [
    "List of python package to be installed:\n",
    "\n",
    "1. numpy\n",
    "2. pytorch\n",
    "3. pandas\n",
    "4. pycox\n",
    "5. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87a740c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from dataset.syndata.syndata_v01 import DGP\n",
    "from model.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5add1",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f800926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum censored instances are necessary to conduct predictive performance comparison with other methods\n",
    "synthetic = DGP(lambda_1 = 1.5, lambda_2 = 0.5)\n",
    "df =  synthetic.generate_samples(N = 3000, random_state = 13, p_censor = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e17804",
   "metadata": {},
   "source": [
    "Spcify each column in relation to the generative process. Each variable indicates the following:\n",
    "\n",
    "1. t_cols: survival time\n",
    "2. s_cols: (failure) event (0: right-censored, 1: death)\n",
    "3. c_cols: (continuous) physiological measurements\n",
    "4. x_cols: (binary) morbidity indicators\n",
    "5. a_cols: (continuous) age\n",
    "6. b_cols: (both continuous and discrete) personal background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa1262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cols = ['durations'] \n",
    "s_cols = ['events']\n",
    "c_cols = ['c']\n",
    "b_cols = ['b']\n",
    "a_cols = ['a']\n",
    "x_cols = ['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf08fa3",
   "metadata": {},
   "source": [
    "Specify a set of columns, i.e. a set of all continuous variables except for survival time, that we normalise before the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7fbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_std = ['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cea6c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"syndata_v1/custom/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b41a85",
   "metadata": {},
   "source": [
    "Define Experiment clase whose argument includes main dataset (df) and a set of columns defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b149ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(df, \n",
    "                t_cols = t_cols, s_cols = s_cols, c_cols = c_cols,\n",
    "                x_cols = x_cols, a_cols = a_cols, b_cols = b_cols,\n",
    "                       directory = directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15108d",
   "metadata": {},
   "source": [
    "Specify hyperparameters including\n",
    "\n",
    "1. hidden_dim: # of neurons\n",
    "2. lr: learning rate\n",
    "3. n_epochs: # of epochs \n",
    "4. batch_size: batch size\n",
    "5. device: \"cpu\", \"mps\" and \"gpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6f720",
   "metadata": {},
   "source": [
    "Performance evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "265f2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 512\n",
    "lr = 5e-3\n",
    "hidden_dim = 64\n",
    "alpha = 0.7\n",
    "beta = 1.0\n",
    "gamma = 1.0\n",
    "z_dim = 2\n",
    "k_peaks = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811770b6",
   "metadata": {},
   "source": [
    "Train M4VAE and compare its performance against other baseline methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f02caf",
   "metadata": {},
   "source": [
    "Key arguments:\n",
    "\n",
    "return_baseline: $\\textbf{standard Cox, DeepCox, Deep time-dependent Cox, DeepHit, DeSurv and SuMo-Net}$ \n",
    "\n",
    "return_metrics: $\\textbf{c-index, brier score, negative binomial log-likelihood and log-likelihood}$, calibration score $\\textbf{D-calibration}$, \\textbf{silverman test} and \\textbf{diptest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e12c494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder: cpu specified, cpu used\n",
      "Decoder: cpu specified, cpu used\n",
      "Decoder: cpu specified, cpu used\n",
      "\tData set size 2400, batch size 512.\n",
      "\n",
      "\tEpoch:  0. Total loss:    50576.93\n",
      "best_epoch: 0\n",
      "\tEpoch:  0. Total val loss:     5070.46\n",
      "\tEpoch: 10. Total loss:     7773.11\n",
      "best_epoch: 10\n",
      "\tEpoch: 10. Total val loss:      953.61\n",
      "\tEpoch: 20. Total loss:     7234.74\n",
      "best_epoch: 20\n",
      "\tEpoch: 20. Total val loss:      934.89\n",
      "\tEpoch: 30. Total loss:     7114.64\n",
      "best_epoch: 30\n",
      "\tEpoch: 30. Total val loss:      912.70\n",
      "\tEpoch: 40. Total loss:     7084.90\n",
      "best_epoch: 40\n",
      "\tEpoch: 40. Total val loss:      887.45\n",
      "\tEpoch: 50. Total loss:     7198.91\n",
      "\tEpoch: 50. Total val loss:      899.36\n",
      "\tEpoch: 60. Total loss:     7057.06\n",
      "best_epoch: 60\n",
      "\tEpoch: 60. Total val loss:      877.59\n",
      "\tEpoch: 70. Total loss:     7052.91\n",
      "\tEpoch: 70. Total val loss:      900.92\n",
      "\tEpoch: 80. Total loss:     7033.14\n",
      "\tEpoch: 80. Total val loss:      890.50\n",
      "\tEpoch: 90. Total loss:     7105.73\n",
      "\tEpoch: 90. Total val loss:      896.16\n",
      "\tEpoch: 100. Total loss:     6979.27\n",
      "\tEpoch: 100. Total val loss:      887.88\n",
      "\tEpoch: 110. Total loss:     6975.25\n",
      "\tEpoch: 110. Total val loss:      914.55\n",
      "\tEpoch: 120. Total loss:     7057.10\n",
      "best_epoch: 120\n",
      "\tEpoch: 120. Total val loss:      869.43\n",
      "\tEpoch: 130. Total loss:     6885.85\n",
      "\tEpoch: 130. Total val loss:      883.86\n",
      "\tEpoch: 140. Total loss:     6877.15\n",
      "\tEpoch: 140. Total val loss:      888.56\n",
      "\tEpoch: 150. Total loss:     6877.39\n",
      "best_epoch: 150\n",
      "\tEpoch: 150. Total val loss:      863.99\n",
      "\tEpoch: 160. Total loss:     6905.24\n",
      "\tEpoch: 160. Total val loss:      879.20\n",
      "\tEpoch: 170. Total loss:     6867.46\n",
      "best_epoch: 170\n",
      "\tEpoch: 170. Total val loss:      861.37\n",
      "\tEpoch: 180. Total loss:     6897.36\n",
      "\tEpoch: 180. Total val loss:      918.14\n",
      "\tEpoch: 190. Total loss:     6792.95\n",
      "best_epoch: 190\n",
      "\tEpoch: 190. Total val loss:      854.02\n",
      "\tEpoch: 200. Total loss:     6743.34\n",
      "\tEpoch: 200. Total val loss:      916.67\n",
      "\tEpoch: 210. Total loss:     6775.25\n",
      "best_epoch: 210\n",
      "\tEpoch: 210. Total val loss:      837.08\n",
      "\tEpoch: 220. Total loss:     6780.06\n",
      "\tEpoch: 220. Total val loss:      863.04\n",
      "\tEpoch: 230. Total loss:     6692.67\n",
      "\tEpoch: 230. Total val loss:      892.28\n",
      "\tEpoch: 240. Total loss:     6752.01\n",
      "\tEpoch: 240. Total val loss:      837.45\n",
      "\tEpoch: 250. Total loss:     6772.93\n",
      "\tEpoch: 250. Total val loss:      860.12\n",
      "\tEpoch: 260. Total loss:     6629.79\n",
      "\tEpoch: 260. Total val loss:      868.81\n",
      "\tEpoch: 270. Total loss:     6613.16\n",
      "\tEpoch: 270. Total val loss:      861.99\n",
      "\tEpoch: 280. Total loss:     6607.51\n",
      "\tEpoch: 280. Total val loss:      859.12\n",
      "\tEpoch: 290. Total loss:     6792.48\n",
      "best_epoch: 290\n",
      "\tEpoch: 290. Total val loss:      825.33\n",
      "\tEpoch: 300. Total loss:     6540.15\n",
      "best_epoch: 300\n",
      "\tEpoch: 300. Total val loss:      809.41\n",
      "\tEpoch: 310. Total loss:     6382.88\n",
      "\tEpoch: 310. Total val loss:      828.55\n",
      "\tEpoch: 320. Total loss:     6615.38\n",
      "\tEpoch: 320. Total val loss:      823.66\n",
      "\tEpoch: 330. Total loss:     6533.62\n",
      "\tEpoch: 330. Total val loss:      854.78\n",
      "\tEpoch: 340. Total loss:     6423.78\n",
      "\tEpoch: 340. Total val loss:      813.13\n",
      "\tEpoch: 350. Total loss:     6377.23\n",
      "best_epoch: 350\n",
      "\tEpoch: 350. Total val loss:      803.00\n",
      "\tEpoch: 360. Total loss:     6315.00\n",
      "best_epoch: 360\n",
      "\tEpoch: 360. Total val loss:      802.72\n",
      "\tEpoch: 370. Total loss:     6309.92\n",
      "\tEpoch: 370. Total val loss:      811.95\n",
      "\tEpoch: 380. Total loss:     6322.77\n",
      "\tEpoch: 380. Total val loss:      823.05\n",
      "\tEpoch: 390. Total loss:     6251.66\n",
      "best_epoch: 390\n",
      "\tEpoch: 390. Total val loss:      776.42\n",
      "\tEpoch: 400. Total loss:     6226.68\n",
      "\tEpoch: 400. Total val loss:      794.00\n",
      "\tEpoch: 410. Total loss:     6165.34\n",
      "best_epoch: 410\n",
      "\tEpoch: 410. Total val loss:      750.42\n",
      "\tEpoch: 420. Total loss:     6232.23\n",
      "\tEpoch: 420. Total val loss:      786.49\n",
      "\tEpoch: 430. Total loss:     6154.98\n",
      "\tEpoch: 430. Total val loss:      753.80\n",
      "\tEpoch: 440. Total loss:     6124.85\n",
      "best_epoch: 440\n",
      "\tEpoch: 440. Total val loss:      739.49\n",
      "\tEpoch: 450. Total loss:     6026.52\n",
      "\tEpoch: 450. Total val loss:      767.35\n",
      "\tEpoch: 460. Total loss:     5999.42\n",
      "\tEpoch: 460. Total val loss:      758.21\n",
      "\tEpoch: 470. Total loss:     5951.89\n",
      "\tEpoch: 470. Total val loss:      778.64\n",
      "\tEpoch: 480. Total loss:     5981.37\n",
      "\tEpoch: 480. Total val loss:      756.29\n",
      "\tEpoch: 490. Total loss:     5888.11\n",
      "\tEpoch: 490. Total val loss:      777.26\n",
      "\tEpoch: 500. Total loss:     5855.79\n",
      "\tEpoch: 500. Total val loss:      752.09\n",
      "\tEpoch: 510. Total loss:     5764.62\n",
      "best_epoch: 510\n",
      "\tEpoch: 510. Total val loss:      723.42\n",
      "\tEpoch: 520. Total loss:     5806.68\n",
      "\tEpoch: 520. Total val loss:      757.14\n",
      "\tEpoch: 530. Total loss:     5680.14\n",
      "best_epoch: 530\n",
      "\tEpoch: 530. Total val loss:      706.80\n",
      "\tEpoch: 540. Total loss:     5610.57\n",
      "best_epoch: 540\n",
      "\tEpoch: 540. Total val loss:      701.70\n",
      "\tEpoch: 550. Total loss:     5568.29\n",
      "best_epoch: 550\n",
      "\tEpoch: 550. Total val loss:      697.01\n",
      "\tEpoch: 560. Total loss:     5555.78\n",
      "best_epoch: 560\n",
      "\tEpoch: 560. Total val loss:      692.92\n",
      "\tEpoch: 570. Total loss:     5520.97\n",
      "\tEpoch: 570. Total val loss:      696.99\n",
      "\tEpoch: 580. Total loss:     5489.19\n",
      "\tEpoch: 580. Total val loss:      695.42\n",
      "\tEpoch: 590. Total loss:     5500.16\n",
      "\tEpoch: 590. Total val loss:      695.64\n",
      "\tEpoch: 600. Total loss:     5494.90\n",
      "\tEpoch: 600. Total val loss:      696.92\n",
      "\tEpoch: 610. Total loss:     5493.95\n",
      "best_epoch: 610\n",
      "\tEpoch: 610. Total val loss:      692.92\n",
      "\tEpoch: 620. Total loss:     5501.10\n",
      "\tEpoch: 620. Total val loss:      694.35\n",
      "\tEpoch: 630. Total loss:     5494.88\n",
      "best_epoch: 630\n",
      "\tEpoch: 630. Total val loss:      690.98\n",
      "\tEpoch: 640. Total loss:     5476.73\n",
      "\tEpoch: 640. Total val loss:      693.37\n",
      "\tEpoch: 650. Total loss:     5464.33\n",
      "\tEpoch: 650. Total val loss:      695.78\n",
      "\tEpoch: 660. Total loss:     5483.48\n",
      "best_epoch: 660\n",
      "\tEpoch: 660. Total val loss:      689.99\n",
      "\tEpoch: 670. Total loss:     5468.76\n",
      "best_epoch: 670\n",
      "\tEpoch: 670. Total val loss:      688.13\n",
      "\tEpoch: 680. Total loss:     5494.12\n",
      "\tEpoch: 680. Total val loss:      693.90\n",
      "\tEpoch: 690. Total loss:     5481.10\n",
      "best_epoch: 690\n",
      "\tEpoch: 690. Total val loss:      688.05\n",
      "\tEpoch: 700. Total loss:     5482.29\n",
      "\tEpoch: 700. Total val loss:      694.22\n",
      "\tEpoch: 710. Total loss:     5481.28\n",
      "\tEpoch: 710. Total val loss:      692.60\n",
      "\tEpoch: 720. Total loss:     5479.85\n",
      "\tEpoch: 720. Total val loss:      696.12\n",
      "\tEpoch: 730. Total loss:     5475.53\n",
      "\tEpoch: 730. Total val loss:      694.88\n",
      "\tEpoch: 740. Total loss:     5489.37\n",
      "\tEpoch: 740. Total val loss:      694.52\n",
      "\tEpoch: 750. Total loss:     5470.05\n",
      "\tEpoch: 750. Total val loss:      689.12\n",
      "\tEpoch: 760. Total loss:     5482.57\n",
      "\tEpoch: 760. Total val loss:      691.81\n",
      "\tEpoch: 770. Total loss:     5470.30\n",
      "\tEpoch: 770. Total val loss:      698.25\n",
      "\tEpoch: 780. Total loss:     5476.64\n",
      "\tEpoch: 780. Total val loss:      702.41\n",
      "\tEpoch: 790. Total loss:     5477.06\n",
      "\tEpoch: 790. Total val loss:      697.35\n",
      "\tEpoch: 800. Total loss:     5466.68\n",
      "\tEpoch: 800. Total val loss:      691.65\n",
      "\tEpoch: 810. Total loss:     5465.23\n",
      "\tEpoch: 810. Total val loss:      693.42\n",
      "\tEpoch: 820. Total loss:     5476.24\n",
      "\tEpoch: 820. Total val loss:      694.63\n",
      "\tEpoch: 830. Total loss:     5475.51\n",
      "\tEpoch: 830. Total val loss:      688.55\n",
      "\tEpoch: 840. Total loss:     5470.91\n",
      "\tEpoch: 840. Total val loss:      696.00\n",
      "\tEpoch: 850. Total loss:     5470.90\n",
      "\tEpoch: 850. Total val loss:      691.97\n",
      "\tEpoch: 860. Total loss:     5477.15\n",
      "\tEpoch: 860. Total val loss:      691.82\n",
      "\tEpoch: 870. Total loss:     5485.43\n",
      "\tEpoch: 870. Total val loss:      690.91\n",
      "\tEpoch: 880. Total loss:     5482.55\n",
      "\tEpoch: 880. Total val loss:      696.21\n",
      "\tEpoch: 890. Total loss:     5468.91\n",
      "\tEpoch: 890. Total val loss:      693.99\n",
      "\tEpoch: 900. Total loss:     5494.10\n",
      "\tEpoch: 900. Total val loss:      691.06\n",
      "\tEpoch: 910. Total loss:     5470.13\n",
      "\tEpoch: 910. Total val loss:      694.35\n",
      "\tEpoch: 920. Total loss:     5479.71\n",
      "\tEpoch: 920. Total val loss:      697.25\n",
      "\tEpoch: 930. Total loss:     5472.34\n",
      "\tEpoch: 930. Total val loss:      692.46\n",
      "\tEpoch: 940. Total loss:     5482.03\n",
      "\tEpoch: 940. Total val loss:      705.67\n",
      "\tEpoch: 950. Total loss:     5472.79\n",
      "\tEpoch: 950. Total val loss:      694.28\n",
      "\tEpoch: 960. Total loss:     5470.22\n",
      "\tEpoch: 960. Total val loss:      696.46\n",
      "\tEpoch: 970. Total loss:     5477.28\n",
      "\tEpoch: 970. Total val loss:      700.36\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tEpoch: 980. Total loss:     5477.85\n",
      "\tEpoch: 980. Total val loss:      695.91\n",
      "\tEpoch: 990. Total loss:     5475.11\n",
      "\tEpoch: 990. Total val loss:      691.55\n",
      "loading low_\n"
     ]
    }
   ],
   "source": [
    "experiment.train(cols_to_std = cols_to_std, z_dim = z_dim, hidden_dim = hidden_dim, alpha =alpha, beta = beta, \n",
    "                 gamma = gamma, lr = lr, n_epochs = n_epochs, batch_size = batch_size, \n",
    "                 logging_freq = 10, max_wait = 20, device = 'cpu', k_peaks = k_peaks, verbose = True, \n",
    "                 return_baseline = False, return_metrics = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "94072964",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summary_data( experiment ):\n",
    "    obs, cens = experiment.df[experiment.events].value_counts()\n",
    "\n",
    "    print(f\"Observed: {obs} ({np.round(obs / (obs + cens), 2)} )\")\n",
    "    print(f\"Censored: {cens}, ({np.round(cens / (obs + cens), 2)})\")\n",
    "    \n",
    "    b_dim = len(experiment.bidx) + len(experiment.aidx)\n",
    "    c_dim = experiment.c_dim\n",
    "    x_dim = len(experiment.xidx)\n",
    "    \n",
    "    print(f\"(Continuous) covariates: {c_dim}\")\n",
    "    print(f\"(Binary) covariates: {x_dim}\")\n",
    "    print(f\"Auxiliary covariates: {b_dim}\")\n",
    "    \n",
    "    event_subset = experiment.df[experiment.df[experiment.events] == 1]\n",
    "    t_mean, t_max = event_subset[experiment.durations].mean(), event_subset[experiment.durations].max()\n",
    "    \n",
    "    print(f\"Event time / Mean: {np.round(t_mean, 1)}\")\n",
    "    print(f\"Event time / Max: {np.round(t_max, 1)}\")\n",
    "    \n",
    "    censor_subset = experiment.df[experiment.df[experiment.events] == 0]\n",
    "    s_mean, s_max = censor_subset[experiment.durations].mean(), censor_subset[experiment.durations].max()\n",
    "    \n",
    "    print(f\"Censoring time / Mean: {np.round(s_mean, 1)}\")\n",
    "    print(f\"Censoring time / Max: {np.round(s_max, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ad7e473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed: 2699 (0.9 )\n",
      "Censored: 301, (0.1)\n",
      "(Continuous) covariates: 1\n",
      "(Binary) covariates: 1\n",
      "Auxiliary covariates: 2\n",
      "Event time / Mean: 0.9\n",
      "Event time / Max: 3.8\n",
      "Censoring time / Mean: 0.5\n",
      "Censoring time / Max: 2.4\n"
     ]
    }
   ],
   "source": [
    "summary_data(experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
