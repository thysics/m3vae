{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "159f3f63",
   "metadata": {},
   "source": [
    "List of python package to be installed:\n",
    "\n",
    "1. numpy\n",
    "2. pytorch\n",
    "3. pandas\n",
    "4. pycox\n",
    "5. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a740c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from dataset.syndata.syndata_v06 import DGP\n",
    "from model.experiment import Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b5add1",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f800926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# minimum censored instances are necessary to conduct predictive performance comparison with other methods\n",
    "synthetic = DGP(lambda_1 = 1.5, lambda_2 = 0.5)\n",
    "df =  synthetic.generate_samples(N = 3000, random_state = 13, p_censor = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e17804",
   "metadata": {},
   "source": [
    "Spcify each column in relation to the generative process. Each variable indicates the following:\n",
    "\n",
    "1. t_cols: survival time\n",
    "2. s_cols: (failure) event (0: right-censored, 1: death)\n",
    "3. c_cols: (continuous) physiological measurements\n",
    "4. x_cols: (binary) morbidity indicators\n",
    "5. a_cols: (continuous) age\n",
    "6. b_cols: (both continuous and discrete) personal background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1262b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_cols = ['durations'] \n",
    "s_cols = ['events']\n",
    "c_cols = ['c']\n",
    "b_cols = ['b']\n",
    "a_cols = ['a']\n",
    "x_cols = ['x']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf08fa3",
   "metadata": {},
   "source": [
    "Specify a set of columns, i.e. a set of all continuous variables except for survival time, that we normalise before the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7fbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_std = ['c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea6c232",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"syndata_v1/k_fold_cv/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b41a85",
   "metadata": {},
   "source": [
    "Define Experiment clase whose argument includes main dataset (df) and a set of columns defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b149ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = Experiment(df, \n",
    "                t_cols = t_cols, s_cols = s_cols, c_cols = c_cols,\n",
    "                x_cols = x_cols, a_cols = a_cols, b_cols = b_cols,\n",
    "                       directory = directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15108d",
   "metadata": {},
   "source": [
    "Specify hyperparameters including\n",
    "\n",
    "1. hidden_dim: # of neurons\n",
    "2. lr: learning rate\n",
    "3. n_epochs: # of epochs \n",
    "4. batch_size: batch size\n",
    "5. device: \"cpu\", \"mps\" and \"gpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd6f720",
   "metadata": {},
   "source": [
    "Performance evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265f2313",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 512\n",
    "lr = 5e-3\n",
    "hidden_dim = 64\n",
    "alpha = 0.7\n",
    "beta = 1.0\n",
    "gamma = [1.0, 10.0]\n",
    "z_dim = 2\n",
    "k_peaks = 1\n",
    "fold = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811770b6",
   "metadata": {},
   "source": [
    "Train M4VAE and compare its predictive performance against other baseline methods, using K-fold cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f02caf",
   "metadata": {},
   "source": [
    "Key arguments:\n",
    "\n",
    "return_baseline: $\\textbf{standard Cox, DeepCox, Deep time-dependent Cox, DeepHit, DeSurv and SuMo-Net}$ \n",
    "\n",
    "return_metrics: $\\textbf{c-index, brier score, negative binomial log-likelihood and log-likelihood}$, calibration score $\\textbf{D-calibration}$, \\textbf{silverman test} and \\textbf{diptest}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12c494",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "experiment.run_evaluations_by_fold(cols_to_std = cols_to_std, num_folds = fold, v_prop = 0.1, random_state = 13, \n",
    "                                    hidden_dim = hidden_dim, z_dim = z_dim, alpha = alpha, beta = beta, gamma = gamma, \n",
    "                                    lr = lr, n_epochs = n_epochs, batch_size = batch_size, logging_freq = 10, max_wait = 20, \n",
    "                                     device = 'cpu', return_baseline = True, verbose = False, k_peaks = k_peaks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e2501f",
   "metadata": {},
   "source": [
    "Present outcome that is comprised of\n",
    "1. eval_dict: evaluation metrics described above\n",
    "2. surv_dict: predictive survival curve inferred by each model and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01012145",
   "metadata": {},
   "outputs": [],
   "source": [
    "surv_dict = torch.load(\"result/\" + directory + f\"surv_dict_10_fold\")\n",
    "eval_dict = torch.load(\"result/\" + directory + f\"eval_dict_10_fold\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5af4ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "for metric in eval_dict.keys():\n",
    "    print(f\"\\n{metric}\")\n",
    "    for model in eval_dict[metric].keys():\n",
    "        mean = np.mean(eval_dict[metric][model]).round(3)\n",
    "        print(f\"{model}: {np.quantile(a = eval_dict[metric][model], q = [0.025, 0.975], method = 'closest_observation').round(3)}, Mean: {mean}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3f05db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for metric in eval_dict.keys():\n",
    "    print(f\"\\n{metric}\")\n",
    "    for model in eval_dict[metric].keys():\n",
    "        mean = np.mean(eval_dict[metric][model]).round(3)\n",
    "        std = np.std(eval_dict[metric][model]).round(3)\n",
    "        print(f\"{model}: {mean} $\\pm$ {std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24aab0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "def show_pred_curve( i, surv_dict, figsize = (6, 4) ):\n",
    "    \"\"\"\n",
    "        i: patient index\n",
    "    \"\"\"\n",
    "    plt.figure(figsize = figsize)\n",
    "    for key in surv_dict.keys():\n",
    "        event = surv_dict['Test'].iloc[i, :]['events']\n",
    "        time = surv_dict['Test'].iloc[i, :]['durations']\n",
    "        t = np.round(time, 3)\n",
    "        \n",
    "        if key != \"Test\":\n",
    "            plt.plot(surv_dict[key].iloc[:, i], label = key)\n",
    "        \n",
    "            \n",
    "        plt.xlim([0, surv_dict['Test']['durations'].max()])\n",
    "        plt.xlabel(\"Time-to-failure\")\n",
    "        plt.ylabel(\"Survival probability\")\n",
    "    \n",
    "    plt.axvline(t, color = 'black', linestyle = 'dashed', label = f't = %.2f ' % t + f'| s = {event}')\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e913a926",
   "metadata": {},
   "source": [
    "Show predictive survival curves inferred by every model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fabe0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred_curve(6, surv_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bbc884",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_pred_curve(50, surv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35121b0",
   "metadata": {},
   "source": [
    "Present evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e529767f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "experiment.model_selections(cols_to_std = cols_to_std, hidden_dim = hidden_dim, lr = lr, \n",
    "                    n_epochs = n_epochs, batch_size = batch_size, device = device,\n",
    "                           z_grid = [2, 10], alpha_grid = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], \n",
    "                           beta_grid = [1.0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3097a49",
   "metadata": {},
   "source": [
    "It runs a set of benchmark models including $\\textbf{standard Cox, DeepCox, Deep time-dependent Cox, DeepHit, DeSurv and SuMo-Net}$ to the dataset and save their predictive performance in terms of $\\textbf{c-index, brier score, negative binomial log-likelihood and log-likelihood}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94072964",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary_data( experiment ):\n",
    "    obs, cens = experiment.df[experiment.events].value_counts()\n",
    "\n",
    "    print(f\"Observed: {obs} ({np.round(obs / (obs + cens), 2)} )\")\n",
    "    print(f\"Censored: {cens}, ({np.round(cens / (obs + cens), 2)})\")\n",
    "    \n",
    "    b_dim = len(experiment.bidx) + len(experiment.aidx)\n",
    "    c_dim = experiment.c_dim\n",
    "    x_dim = len(experiment.xidx)\n",
    "    \n",
    "    print(f\"(Continuous) covariates: {c_dim}\")\n",
    "    print(f\"(Binary) covariates: {x_dim}\")\n",
    "    print(f\"Auxiliary covariates: {b_dim}\")\n",
    "    \n",
    "    event_subset = experiment.df[experiment.df[experiment.events] == 1]\n",
    "    t_mean, t_max = event_subset[experiment.durations].mean(), event_subset[experiment.durations].max()\n",
    "    \n",
    "    print(f\"Event time / Mean: {np.round(t_mean, 1)}\")\n",
    "    print(f\"Event time / Max: {np.round(t_max, 1)}\")\n",
    "    \n",
    "    censor_subset = experiment.df[experiment.df[experiment.events] == 0]\n",
    "    s_mean, s_max = censor_subset[experiment.durations].mean(), censor_subset[experiment.durations].max()\n",
    "    \n",
    "    print(f\"Censoring time / Mean: {np.round(s_mean, 1)}\")\n",
    "    print(f\"Censoring time / Max: {np.round(s_max, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad7e473",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_data(experiment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
